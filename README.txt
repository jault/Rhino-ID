This is a project for the Ol Pejeta Conservancy as described at [https://www.mindsumo.com/contests/track-endangered-rhinos-with-an-algorithmic-visual-tracking-system]. The objective is to identify animals in the conservancy from video collected by an automated drone. The Java JRE is required to run the application, find the latest version at [http://www.oracle.com/technetwork/java/javase/downloads/index.html?ssSourceSiteId=otnjp].

My project grew too large to fit into a single 10MB compressed file while including the referenced libraries. I have packaged the full Eclipse workspace for ease of access. Please find the full source at []. As well an executable of the program can be found at []. You may need to reset the references to the libraries in the project's build path. This project has represented a considerable amount of my time, as such I hope you'll give it the consideration that might entail. If there are issues, compiling or otherwise, please contact me directly at [jault0@gmail.com]. If selected I would be more than willing to modify and update the program to desired specifications.

The algorithm I employed works as following. The video is first processed to find regions of interest (possible animals). This is done by taking a locally adaptive threshold over each frame and selecting continuous regions in which the luminosity is greater than the surrounding area by some factor. The features of these regions are then compared to a stored training set to classify the region as animal, or non-animal.

There are a few shortcomings of this approach at the moment. The most important of is the frames selected are not continuous but at an arbitrary interval. The reason for this is that the test video provided allowed the camera on the drone to pan. The problem with this is that there is no way to predict the motion of the camera and therefore animals would very likely frequently be counted more than once resulting in an inaccurate census. Currently the algorithm makes no attempt at discerning between different species of animal or making use of the visual data. It could be expanded to work with the visual data to identify species, however this has not been implemented as the visual data provided did not match the IR data provided. Upon running the program the user is given 5 choices. First a video should be selected via the "Select Video" button. Then "Process Video" should be allowed ample time to process the entire video. This may take a very long amount of time on some computers. It may display as non-responsive, it is still processing the video. The remaining choices are "Training Mode", "Auto-Analysis Mode", and "Options".

Training Mode allows the user to add to the trained data set for the auto-analysis to work off of. Some training data must be present before auto-analysis can be run. Typically training sets are very large, the larger the trained set the better the auto-analysis will classify animals. This mode also performs the basic functionality desired in bracketing potential animals.

Auto-Analysis Mode will classify the regions of interest in a video automatically and display the results to the user to review. Due to the limited amount of test data this feature severely lacked testing and may not give the expected results as is. With access to more data this could be refined.

When the options menu is displayed there are currently four fields; "Local", "Brightness", "Method", and "FPS".

Local sets the neighborhood size in pixels for the adaptive threshold to search. This is the single largest factor in performance. Performance decreases exponentially with an increase in local size. The appropriate value will depend on the dimensions of the video input. Testing was done with a 640x480 resolution video, values between 20-50 are recommended for this.

Brightness sets the luminosity in which the adaptive neighborhood must exceed to be considered a region of interest. Typically a value of 40% - 60% will yield the best results. If the IR data is unusually bright a lower value may be desired.

Method determines how the adaptive threshold calculates the statistical properties of it's neighborhood. Currently only Mean and Min/Max are functioning.

FPS sets the number of frames to analyze per second. Please review the shortcomings of the algorithm to understand why this is necessary.